# CS 240 Enriched
## Data Structures and Data Management
#### 1/8/2015
Elvin Yung

### Timing of programs
* We have a timing function `T: Input -> R+` (or `T_p`) for each program
* The reason why these timing functions aren't effective is because computer hardware always improve. Therefore the timing functions are poorly defined.
* Therefore we can't use "time" *per se* to time our programs, since it is not a universal measure.
* So we count the number of elementary operations performed by the program. 

* A **RAM**, or random access machine, is a central processor with a finite set of operations on fixd width and running in at most *c* clock cycles each. 
* There are two main instruction sets: **CISC** (complex instruction set computing), and **RISC** (reduced instruction set computing).
* there should be more things here 
* So the running time of some program is the count of the number of operations, including CPU instructions, and memory instructions. 

We can count the running time of a program with this method:
1. Write algorithms in pesudocode.
2. Count the number of primitive operations. 

* We can compare the running times of two different programs/algorithms by plotting the timing function in one graph. 
* Since the input itself isn't plottable, we substitute the input with the input size. 
* Since the input size might be associated with multiple inputs, we take some one metric from all the running time data associated with that one input size (e.g. worst case, average case, etc).
* Essentially, at the end we have some timing function `T_A: N -> R+` for some algorithm `A`, and `T(n) = max(T_A(I) | |I| = n)`.
* Then for example, for merge sort, `T(n) = n log n`.

### Order notation
* Order notation is a way to compare functions. 
* We introduce the order notation for some function `f`: `f(x) = O(g(x))` 
* **Big-O:** `f(n)` is `O(g(n))` if there exists constants `c > 0` and `n_0 > 0` such that `0 <= f(n) <= cg(n)` for all `n >= n_0`.
* Essentially, this means that `f(n)` is `O(g(n))` if eventually, `g(n)` is greater than `f(n)`. In other words, eventually, `f` will grow slower than `g`.
* **Big Omega:** `f(n) is `Ω(g(n))` if there exists constants `c > 0` and `n_0 > 0` such that `0 <= cg(n) <= f(n)` for all `n >= n_0`. 
* **Big Theta:** `f(n) is `Ө(g(n))` if there exists constants `c_1 > 0`, `c_2 > 0` and `n_0 > 0` such that `0 <= c_1(g(n) <= f(n) <= c_2*g(n)` for all `n >= n_0`. 
* **Small O:** `f(n) is `o(g(n))` if there exists constants `c > 0` and `n_0 > 0` such that `0 <= f(n) < cg(n)` for all `n >= n_0`.
* **Small Omega:** `f(n) is `w(g(n))` if there exists constants `c > 0` and `n_0 > 0` such that `0 <= cg(n) < f(n)` for all `n >= n_0`. 


These are all the orders:

| Order | Order in R | Order in function | Meaning |
|---|------------|-------------------|---------|
| O | `x <= y`   | `f(x) = O(g(x))`  | `f(x)` eventually grows slower than `g(x)`|
| Ω | `x >= y`   | `f(x) = Ω(g(x))`  | `f(x)` eventually grows faster than `g(x)`|
| Ө | `x = y`    | `f(x) = Ө(g(x))`  | `f(x)` grows at roughly the same rate as `g(x)`|
| o | `x < y`    | `f(x) = o(g(x))`  | `f(x)` eventually grows strictly slower than `g(x)`|
| w | `x > y`    | `f(x) = w(g(x))`  | `f(x)` eventually grows strictly faster than `g(x)`|

