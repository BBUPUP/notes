# Touchless Interfaces

CS 349 - User interfaces, LEC 001

6-27-2016

Elvin Yung

[Slides](https://www.student.cs.uwaterloo.ca/~cs349/s16/slides/9.1-touchless_interfaces.pdf)


* In many ways, computing has changed a lot in the last few years. A major change is that now we put sensors in everything.
* In the phones on our pocket, there are now not only cameras and microphones, but there might now be things like accelerometers, gyroscopes, and magnetometers.
* There are challenges to working with the data.
* You need to figure out what the data means - how to calibrate and interpret the data.
* Data might also be noisy - if you plot something like gyroscope data from someone's phone, you'll probably get a very jittery graph.
* So you need to somehow normalize the raw data.

## Types of Data
* This is more of a wishlist.
* A lot of the stuff that *do* currently exist in everyday devices, we don't do a lot with. There's a lot of research on it, but they haven't been applied to real life yet.

![](https://i.imgur.com/35r5iv9.png)

## Using Data
* We'll be talking about the first two items the most.

![](https://i.imgur.com/jOv98pn.png)

* More on the third point: authentication is important.
* More on the fifth point: It seems like science fiction, but we can actually do a lot of this already, and there are a lot of real and tangible uses.

## Design Considerations
* Why don't we do stuff like this all the time?
  * Because it's expensive.
  * Things like computer vision are really computationally intensive, and still can't be easily done in real time.

* How do you use all this data?

## Implementing Signal-to-Command Systems
* We probably already all know this is called **machine learning**.
* The objective is basically to get your source data into something manageable, and then to build mathematical models around it.

* First, you **preprocess** your data. (compress, smooth, downsample, etc.)
* Then, you **select features** - extract exactly what you need from the data.
  * For example, for face detection, I don't actually need the entire picture, just things like the distances between the eyes, etc.
* Finally you perform **classification** - figure out which category the specific input belongs to.

## Example: Lights
* Based on [this study](http://research.microsoft.com/en-us/um/redmond/groups/coet/homes/interact2001/paper.pdf) by Burmitt et al.
* Situation: there are a bunch of lights in the room. What is the best way to control it?
* Turns out that most people prefer to use voice, but it's too imprecise.

## Challenges in Building Touchless Interfaces
* Consider **explicit interaction** - explicit commands that the user makes to the system, and expects to get direct feedback.
* Consider **implicit interaction** - the system is always on, monitors the relevant data, and reacts to changes. Basically, a system that watches you and learns from you.

### Errors
You need to consider:
* False positives: When the system interprets something as a positive input, when it shouldn't have been. Triggered by *high* sensitivity.
* False negatives: when the user thinks they did something, but the system doesn't detect it. Triggered by *low* sensitivity.

* Users want to feel that they have control. [Do What I Mean](https://en.wikipedia.org/wiki/DWIM)
* Users are intolerant of errors.

#### Strategies to Deal with Errors
* Graceful degradation: work with the closest possible interpretation of input
* Given ambiguity, prompt with "did you mean this?" etc.
* Let users query the system to figure out what exactly happened when something goes wrong
* The problem with a lot of this is that since the whole point is the user doesn't interact with the GUI, a lot of this is actually pretty hard to do.
