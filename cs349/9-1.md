# Touchless Interfaces

CS 349 - User interfaces, LEC 001

6-27-2016

Elvin Yung

[Slides](https://www.student.cs.uwaterloo.ca/~cs349/s16/slides/9.1-touchless_interfaces.pdf)


* In many ways, computing has changed a lot in the last few years. A major change is that now we put sensors in everything.
* In the phones on our pocket, there are now not only cameras and microphones, but there might now be things like accelerometers, gyroscopes, and magnetometers.
* There are challenges to working with the data.
* You need to figure out what the data means - how to calibrate and interpret the data.
* Data might also be noisy - if you plot something like gyroscope data from someone's phone, you'll probably get a very jittery graph.
* So you need to somehow normalize the raw data.

## Types of Data
* This is more of a wishlist.
* A lot of the stuff that *do* currently exist in everyday devices, we don't do a lot with. There's a lot of research on it, but they haven't been applied to real life yet.

![](https://i.imgur.com/35r5iv9.png)

## Using Data
* We'll be talking about the first two items the most.

![](https://i.imgur.com/jOv98pn.png)

* More on the third point: authentication is important.
* More on the fifth point: It seems like science fiction, but we can actually do a lot of this already, and there are a lot of real and tangible uses.

## Design Considerations
* Why don't we do stuff like this all the time?
  * Because it's expensive.
  * Things like computer vision are really computationally intensive, and still can't be easily done in real time.

* How do you use all this data?

## Implementing Signal-to-Command Systems
* We probably already all know this is called **machine learning**.
* The objective is basically to get your source data into something manageable, and then to build mathematical models around it.

* First, you **preprocess** your data. (compress, smooth, downsample, etc.)
* Then, you **select features** - extract exactly what you need from the data.
  * For example, for face detection, I don't actually need the entire picture, just things like the distances between the eyes, etc.
* Finally you perform **classification** - figure out which category the specific input belongs to.

## Example: Lights
* Based on [this study](http://research.microsoft.com/en-us/um/redmond/groups/coet/homes/interact2001/paper.pdf) by Burmitt et al.
* Situation: there are a bunch of lights in the room. What is the best way to control it?
* Turns out that most people prefer to use voice, but it's too imprecise.

## Challenges in Building Touchless Interfaces
* Consider **explicit interaction** - explicit commands that the user makes to the system, and expects to get direct feedback.
* Consider **implicit interaction** - the system is always on, monitors the relevant data, and reacts to changes. Basically, a system that watches you and learns from you.

### Errors
You need to consider:
* False positives: When the system interprets something as a positive input, when it shouldn't have been. Triggered by *high* sensitivity.
* False negatives: when the user thinks they did something, but the system doesn't detect it. Triggered by *low* sensitivity.

* Users want to feel that they have control. [Do What I Mean](https://en.wikipedia.org/wiki/DWIM)
* Users are intolerant of errors.

#### Strategies to Deal with Errors
* Graceful degradation: work with the closest possible interpretation of input
* Given ambiguity, prompt with "did you mean this?" etc.
* Let users query the system to figure out what exactly happened when something goes wrong
* The problem with a lot of this is that since the whole point is the user doesn't interact with the GUI, a lot of this is actually pretty hard to do.

### The Live Mic Problem
* [We begin bombing in five minutes](https://en.wikipedia.org/wiki/We_begin_bombing_in_five_minutes)
* When you have a system that's always monitoring for information, it's *always monitoring for information*.
* For something like an in-air gesture system, it's easy for the system to misinterpret the user's movements when they don't mean to be input.

#### Solution
##### Reserved Actions
* Choose a specific set of actions/gestures that are globally used for navigation or commands.
* The obvious problem with this? You might trigger the command even if you didn't want to.
* [Scriboli](http://www.patrickbaudisch.com/projects/scriboli/) - experimental pen input delimiter gesture

* Another example: cheating with an inductive pen input to allow for a hover state ([Hover Widgets](https://www.youtube.com/watch?v=WPbiPn1b1zQ))
* The problem is that every time you move your cursor in an L-shape when it's not touching the screen,
* It's hard for reserved actions not to interfere with other things

##### Delimeters
* A way of segregating input from your commands.
* i.e. some "trigger" gesture to indicate that you want to start or stop recognizing a gesture
* It's also called a *clutch* (but don't use that, because it's also an HCI term for a mouse movement)
* Basically, come up with a completely new gesture that people are unlikely to do in normal situation.

##### Multi-modal Input
* Why does an iPhone have a button in the front?
* It's sort of a way of serving as a delimiter for the iPhone.
* Notice that in the iPhone, other system functions (i.e. volume) are also assigned to hardware buttons.
* The application completely owns the screen and can assign whatever gestures it wants, without being overridden by the system.
* This is the concept of a **multi-modal** input - i.e. having multiple sources of input.
* MIT 1979 - [Put that There](https://www.youtube.com/watch?v=RyBEUyEtxQo)

### Feedback is Useful
* Feedback is brutally hard to do in a gestural system, but it's also super important, because the user needs feedback about their input.
* You *need* to provide feedback - it's the only way a user can be productive.
* [Jump](http://hci.cs.uwaterloo.ca/sites/default/files/jump_GI_2007.pdf) - early AR system that lets user get feedback on the display itself
* In the HCI Lab (DC 3540) there's a bunch of gesture stuff

### More on Speech Interfaces
* Terminology: SUI stands for *speech user interface*.
* Some early research: [SpeechActs](https://www.youtube.com/watch?v=OzNRsGaXyUA) (1995) - figured out that these systems are pretty terrible, even if the recognition rates were high

#### Challenges
* How to context switch? For example, if you're browsing through messages, how do you open a message, reply to it, and then go back?
* How to parse natural language well? We can pull out the words, but still can't figure out what every combination of words means.
* How to navigate a dialogue? Can't just replace dialog boxes with an equivalent voice prompt.

#### Recognition Errors
* User speaks before the system is ready to listen - e.g. instead of saying, "Siri what's on my calendar?", you have to say "Siri <pause> what's on my calendar?"
* System might pick up background noise, and get garbled input
* It's hard to have a rich detailed conversation with a SUI.

There are basically two main types of errors in an SUI:
* **Rejection errors**: the computer doesn't recognize commands
* **Substitution errors**: the computer misinterprets commands

#### Examples
Some examples of new, pretty good systems:
* [Viv](https://www.youtube.com/watch?v=M1ONXea0mXg) - from the creators of Siri
* [Hound](https://www.youtube.com/watch?v=M1ONXea0mXg)
